{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb23c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp2/howard/venv_manager/vl-sft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Mistral3ForConditionalGeneration, FineGrainedFP8Config, AutoProcessor, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd05cbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 585/585 [00:03<00:00, 160.71it/s, Materializing param=model.vision_tower.transformer.layers.23.ffn_nor\n",
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/tmp2/share_data/mistralai--Ministral-3-14B-Instruct-2512/\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = Mistral3ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=FineGrainedFP8Config(dequantize=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f202e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp2/howard/venv_manager/vl-sft/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "peft_model_id = \"/tmp2/howard/vl-sft-ocr/models/mistral_input_size_500/checkpoint-318\"\n",
    "lora_model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dc0bc",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642a25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset - expected format with images\n",
    "raw_train_dataset, raw_val_dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=\"data/input/ocr_test_data=100.json\",\n",
    "    split=[\"train[:90%]\", \"train[90%:]\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881a79a",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df9cebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下是這張圖片的 OCR 結果：\n",
      "\n",
      "---\n",
      "\n",
      "**衛生福利部 令**\n",
      "\n",
      "登文日期：中華民國112年5月10日\n",
      "登文字號：衛部醫字第1121663253號\n",
      "附件：「公共場所應設置急救設備管理辦法」修正條文1份\n",
      "\n",
      "修正「公共場所應設置急救設備管理辦法」。\n",
      "\n",
      "附修正「公共場所應設置急救設備管理辦法」。\n",
      "\n",
      "部長 薛蕙芳\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "topk = 1\n",
    "\n",
    "for i, sample in enumerate(raw_train_dataset):\n",
    "    if i >= topk:\n",
    "        break\n",
    "    image = sample[\"image_path\"]\n",
    "    instruction = \"請給我 OCR 結果\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": instruction}\n",
    "        ]}\n",
    "    ]\n",
    "    # template -> 加入一些 tag string -> token_id\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True, \n",
    "        tokenize=True, return_dict=True, \n",
    "        return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=2000)\n",
    "    decoded_output = tokenizer.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n",
    "    print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b608aed",
   "metadata": {},
   "source": [
    "### Saving for VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bacc01",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type dtype is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m merged_lora_model = merged_lora_model.to(dtype=torch.bfloat16)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 保存模型、tokenizer 和 processor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mmerged_lora_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m tokenizer.save_pretrained(output_dir)\n\u001b[32m     15\u001b[39m processor.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp2/howard/venv_manager/vl-sft/lib/python3.11/site-packages/transformers/modeling_utils.py:3179\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, push_to_hub, max_shard_size, variant, token, save_peft_format, save_original_format, **kwargs)\u001b[39m\n\u001b[32m   3177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_main_process:\n\u001b[32m   3178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded:\n\u001b[32m-> \u001b[39m\u001b[32m3179\u001b[39m         \u001b[43mmodel_to_save\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.can_generate():\n\u001b[32m   3181\u001b[39m         model_to_save.generation_config.save_pretrained(save_directory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp2/howard/venv_manager/vl-sft/lib/python3.11/site-packages/transformers/configuration_utils.py:481\u001b[39m, in \u001b[36mPreTrainedConfig.save_pretrained\u001b[39m\u001b[34m(self, save_directory, push_to_hub, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# If we save using the predefined names, we can load using `from_pretrained`\u001b[39;00m\n\u001b[32m    479\u001b[39m output_config_file = os.path.join(save_directory, CONFIG_NAME)\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_diff\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConfiguration saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m push_to_hub:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp2/howard/venv_manager/vl-sft/lib/python3.11/site-packages/transformers/configuration_utils.py:950\u001b[39m, in \u001b[36mPreTrainedConfig.to_json_file\u001b[39m\u001b[34m(self, json_file_path, use_diff)\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[33;03mSave this instance to a JSON file.\u001b[39;00m\n\u001b[32m    941\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    947\u001b[39m \u001b[33;03m        is serialized to JSON file.\u001b[39;00m\n\u001b[32m    948\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[32m--> \u001b[39m\u001b[32m950\u001b[39m     writer.write(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_diff\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_diff\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp2/howard/venv_manager/vl-sft/lib/python3.11/site-packages/transformers/configuration_utils.py:936\u001b[39m, in \u001b[36mPreTrainedConfig.to_json_string\u001b[39m\u001b[34m(self, use_diff)\u001b[39m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    935\u001b[39m     config_dict = \u001b[38;5;28mself\u001b[39m.to_dict()\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/__init__.py:238\u001b[39m, in \u001b[36mdumps\u001b[39m\u001b[34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mcls\u001b[39m = JSONEncoder\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/encoder.py:202\u001b[39m, in \u001b[36mJSONEncoder.encode\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    200\u001b[39m chunks = \u001b[38;5;28mself\u001b[39m.iterencode(o, _one_shot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     chunks = \u001b[38;5;28mlist\u001b[39m(chunks)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/encoder.py:439\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    437\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    438\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda3/envs/cuda-12.4-py311/lib/python3.11/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type dtype is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "output_dir = \"/tmp2/howard/vl-sft-ocr/models/merged--mistral-sft_input_size=500\"\n",
    "merged_lora_model = lora_model.merge_and_unload()\n",
    "\n",
    "# 清除量化配置以避免序列化問題\n",
    "if hasattr(merged_lora_model.config, 'quantization_config'):\n",
    "    merged_lora_model.config.quantization_config = None\n",
    "\n",
    "# 將模型轉換為 float16 或 bfloat16 以確保兼容性\n",
    "merged_lora_model = merged_lora_model.to(dtype=torch.bfloat16)\n",
    "\n",
    "# 保存模型、tokenizer 和 processor\n",
    "merged_lora_model.save_pretrained(output_dir, safe_serialization=True)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "processor.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d22054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vl-sft)",
   "language": "python",
   "name": "vl-sft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
